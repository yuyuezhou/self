lda.py注释

####路径设置：
    设置了各种所需文件的所在路径
    对于存有待分析文本的data文件进行了读取

####分词函数：
    略

####LDA基本思想分析：
    n_features = 20 所有文章总共的关键词数量
    n_topics = 2 想要得到的主题类别的数量
    n_top_words = 15 每个主题所考虑的关键词数量，
                     或者说根据多少个关键词能分析出主题
                     抑或是认为多少个关键词就可以影响并决定主题
                     (<=n_features 否则不符合常识)
    思路：
    1.根据CountVectorizer()对已经分好词的各文章形成词频矩阵，该函数会将所有文章全部
      的关键词及其编号都存放在其.vocabulary_中。为字典形式:'关键词','编号' 编号范围[0,总个数-1]
      至于编号规则，英文是按字母顺序，中文待查
    2.通过fit_transform函数计算各个词语出现的次数，该函数返回形式：
      (num,i) times : num:文章序号 i:关键词编号 times:对应关键词的出现次数
      如 (2,1) 3 意为：第2篇文章里编号为1的关键词出现了3次
    3.加载LatentDirichletAllocation()函数并设置相关参数，继而利用其.fit()函数进行LDA内部的学习
      通过其内部学习计算得出的结果存放在其.components_中，形式为嵌套列表：每个元素都是列表，每个列表元素都是LDA计算出来的主题，
      列表元素(即主题)里的元素则是LDA计算出来的每一个关键词对当前主题的贡献值(影响力)(正相关)。
      顺序规则：每个列表元素(即主题)里的下标位置即为之前所得关键词的编号，故该下标位置(编号)对应的数值即为对应关键词对当前主题的影响力。
    4.CountVectorizer()中的.get_feature_names_out()存放了所有的关键词(注意与.vocabulary_区分)
      .get_feature_names_out()中的关键词按照.vocabulary_中的编号顺序由小到大排序。
    5.利用enumerate()遍历.components_。返回的索引即为某个主题，返回的值即为该主题对应的主题影响因子列表(见第3条)
      根据设置好的n_top_words来确定本程序认定多少个关键词就可以影响并决定主题。
      利用argsort()结合n_top_words对主题影响因子列表进行从大到小的排序(从大到小是因为影响因子数值越大，对主题影响力越大)
      在排好序的列表中将获取到的前n_top_words个主题影响因子之前所在(即未排序时)的下标传至.get_feature_names_out()的关键词表中查找。
      所得到的一系列关键词即为该主题下的关键词(认为就是这些关键词决定这篇文章的主题)